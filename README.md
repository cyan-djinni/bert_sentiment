â¸»

ğŸ¬ BERT Sentiment Analysis on IMDB

Languages: English | ä¸­æ–‡ | Deutsch | FranÃ§ais

â¸»

Overview

This project fine-tunes the pre-trained transformer model bert-base-uncased for binary sentiment classification on the IMDB movie review dataset.

The project implements a complete NLP pipeline including:
	â€¢	Data preprocessing
	â€¢	Transformer fine-tuning
	â€¢	Model evaluation on validation and test sets
	â€¢	Interactive inference

The final model was trained on the full cleaned dataset using GPU acceleration in Google Colab.

â¸»

Dataset

IMDB Movie Review Dataset (Kaggle version)

Binary sentiment labels:
	â€¢	0 â€” Negative
	â€¢	1 â€” Positive

The dataset was cleaned and tokenized prior to training.

â¸»

Method

Base model: bert-base-uncased
Framework: PyTorch + HuggingFace Transformers

Fine-tuning is performed using supervised learning with a linear classification head on top of the BERT encoder.

Maximum sequence length: 128
Optimization: AdamW
Evaluation metrics: Accuracy and weighted F1-score

â¸»

Environment

conda create -n bert_env python=3.10
conda activate bert_env
pip install -r requirements.txt


â¸»

Training

Local development training:

python src/train.py

Full-scale training was conducted in Google Colab using GPU acceleration.

The trained model checkpoint is stored locally and is not included in this repository due to size constraints.

â¸»

Evaluation

python src/evaluate.py --model_path PATH_TO_CHECKPOINT


â¸»

Final Results

Final model (full dataset, Colab GPU training):
	â€¢	Validation F1: 92.67%
	â€¢	Validation Accuracy: 92.64%
	â€¢	Test F1: 89.28%
	â€¢	Test Accuracy: 89.28%

The slight decrease from validation to test performance indicates mild overfitting but overall strong generalization capability.

â¸»

Inference

python src/predict.py

Example:

Enter a review: This movie was fantastic.
Prediction: positive


â¸»

Project Structure

bert_sentiment/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»



ä¸­æ–‡è¯´æ˜

æœ¬é¡¹ç›®åŸºäºé¢„è®­ç»ƒæ¨¡å‹ bert-base-uncased å¯¹ IMDB ç”µå½±è¯„è®ºè¿›è¡ŒäºŒåˆ†ç±»æƒ…æ„Ÿåˆ†æã€‚

æœ€ç»ˆæ¨¡å‹ä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œå¹¶åœ¨ Google Colab GPU ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚

æœ€ç»ˆç»“æœï¼š
	â€¢	Validation F1: 92.67%
	â€¢	Test F1: 89.28%

éªŒè¯é›†ä¸æµ‹è¯•é›†ä¹‹é—´å­˜åœ¨è½»å¾®æ€§èƒ½ä¸‹é™ï¼Œå±äºæ­£å¸¸æ³›åŒ–å·®å¼‚ï¼Œæ•´ä½“æ¨¡å‹è¡¨ç°ç¨³å®šã€‚

â¸»



Deutsche Version

Dieses Projekt fine-tuned das vortrainierte Modell bert-base-uncased fÃ¼r eine binÃ¤re Sentimentanalyse auf dem IMDB-Datensatz.

Das finale Modell wurde mit dem vollstÃ¤ndigen Datensatz unter Verwendung von GPU-Beschleunigung in Google Colab trainiert.

Ergebnisse:
	â€¢	Validation F1: 92.67%
	â€¢	Test F1: 89.28%

Die leichte Leistungsdifferenz zwischen Validierungs- und Testdaten deutet auf ein mildes Overfitting hin, die GeneralisierungsfÃ¤higkeit bleibt jedoch stabil.

â¸»



Version FranÃ§aise

Ce projet entraÃ®ne le modÃ¨le prÃ©-entraÃ®nÃ© bert-base-uncased pour une classification binaire des sentiments sur le dataset IMDB.

Le modÃ¨le final a Ã©tÃ© entraÃ®nÃ© sur lâ€™ensemble complet des donnÃ©es avec accÃ©lÃ©ration GPU sur Google Colab.

RÃ©sultats finaux :
	â€¢	Validation F1 : 92.67%
	â€¢	Test F1 : 89.28%

La lÃ©gÃ¨re baisse entre validation et test indique un surapprentissage modÃ©rÃ© mais une bonne capacitÃ© de gÃ©nÃ©ralisation.

â¸»
