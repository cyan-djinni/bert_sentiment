# ğŸ¬ BERT Sentiment Analysis on IMDB

**Languages:** English | [ä¸­æ–‡](#chinese) | [Deutsch](#german) | [FranÃ§ais](#french)

---

## ğŸ“Œ Overview

This project fine-tunes the pre-trained transformer model `bert-base-uncased` for **binary sentiment classification** on the IMDB movie review dataset.

The implementation covers a complete NLP pipeline:

- Data preprocessing and cleaning  
- Transformer fine-tuning  
- Validation and test evaluation  
- Interactive inference  

The final model was trained on the **full dataset** using **GPU acceleration in Google Colab**.

---

## ğŸ—‚ Dataset

**IMDB Movie Review Dataset** (Kaggle version)

Binary labels:

- `0` â€” Negative  
- `1` â€” Positive  

Reviews are cleaned and tokenized using the BERT tokenizer with a maximum sequence length of 128.

---

## ğŸ§  Methodology

- **Base Model:** `bert-base-uncased`  
- **Framework:** PyTorch + HuggingFace Transformers  
- **Max Sequence Length:** 128  
- **Optimizer:** AdamW  
- **Evaluation Metrics:** Accuracy, Weighted F1-score  

A linear classification head is fine-tuned on top of the pre-trained BERT encoder.

---

## âš™ï¸ Environment Setup

```bash
conda create -n bert_env python=3.10
conda activate bert_env
pip install -r requirements.txt


â¸»

ğŸ”„ Data Preprocessing

python src/preprocess.py

This step performs dataset cleaning and prepares the data for model training.

â¸»

ğŸš€ Training

Local development training:

python src/train.py

Full-scale training was conducted in Google Colab (GPU) on the complete dataset.

Trained model weights are not included in this repository due to size constraints.

â¸»

ğŸ“Š Evaluation

python src/evaluate.py --model_path PATH_TO_CHECKPOINT

Final Model Performance (Full Dataset, GPU Training)

Metric	Validation	Test
Accuracy	92.64%	89.28%
Weighted F1	92.67%	89.28%

The slight decrease from validation to test performance indicates mild overfitting while maintaining strong generalization ability.

â¸»

ğŸ” Inference

python src/predict.py

Example:

Enter a review: This movie was fantastic.
Prediction: positive


â¸»

ğŸ“ Project Structure

bert_sentiment/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ predict.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»



ä¸­æ–‡è¯´æ˜

æœ¬é¡¹ç›®åŸºäºé¢„è®­ç»ƒæ¨¡å‹ bert-base-uncased å¯¹ IMDB ç”µå½±è¯„è®ºè¿›è¡ŒäºŒåˆ†ç±»æƒ…æ„Ÿåˆ†æã€‚

å®Œæ•´æµç¨‹åŒ…æ‹¬ï¼šæ•°æ®æ¸…æ´—ã€æ¨¡å‹å¾®è°ƒã€éªŒè¯ä¸æµ‹è¯•è¯„ä¼°ã€äº¤äº’å¼é¢„æµ‹ã€‚

æœ€ç»ˆæ¨¡å‹ä½¿ç”¨å®Œæ•´æ•°æ®é›†ï¼Œå¹¶åœ¨ Google Colab GPU ç¯å¢ƒä¸‹è®­ç»ƒã€‚

æœ€ç»ˆç»“æœï¼š
	â€¢	Validation F1: 92.67%
	â€¢	Test F1: 89.28%

éªŒè¯é›†ä¸æµ‹è¯•é›†ä¹‹é—´å­˜åœ¨è½»å¾®æ€§èƒ½å·®å¼‚ï¼Œå±äºæ­£å¸¸æ³›åŒ–ç°è±¡ã€‚

â¸»



Deutsche Version

Dieses Projekt fine-tuned das vortrainierte Modell bert-base-uncased fÃ¼r eine binÃ¤re Sentimentanalyse auf dem IMDB-Datensatz.

Der vollstÃ¤ndige Workflow umfasst Datenvorverarbeitung, Training, Evaluation und Inferenz.

Das finale Modell wurde mit dem kompletten Datensatz unter GPU-Beschleunigung in Google Colab trainiert.

Ergebnisse:
	â€¢	Validation F1: 92.67%
	â€¢	Test F1: 89.28%

Die leichte Differenz zwischen Validierungs- und Testleistung deutet auf mildes Overfitting hin.

â¸»



Version FranÃ§aise

Ce projet entraÃ®ne le modÃ¨le prÃ©-entraÃ®nÃ© bert-base-uncased pour une classification binaire des sentiments sur le dataset IMDB.

Le pipeline comprend le prÃ©traitement des donnÃ©es, lâ€™entraÃ®nement, lâ€™Ã©valuation et lâ€™infÃ©rence.

Le modÃ¨le final a Ã©tÃ© entraÃ®nÃ© sur lâ€™ensemble complet des donnÃ©es avec accÃ©lÃ©ration GPU sur Google Colab.

RÃ©sultats finaux :
	â€¢	Validation F1 : 92.67%
	â€¢	Test F1 : 89.28%

La lÃ©gÃ¨re baisse entre validation et test indique un surapprentissage modÃ©rÃ© avec une bonne capacitÃ© de gÃ©nÃ©ralisation.